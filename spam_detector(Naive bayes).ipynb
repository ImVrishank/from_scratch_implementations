{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "documents used:\n",
    "\n",
    "emails.csv --> https://www.kaggle.com/datasets/jackksoncsie/spam-email-dataset\n",
    "    Not the best dataset to train a naive bayes model, it contains emails with words slightly altered so as to not be detected easily\n",
    "\n",
    "google-10000-english-usa.txt --> https://github.com/first20hours/google-10000-english\n",
    "    this is a dataset of the top 10000 most frequently used english words(it has n = 9989 words). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words \n",
    "--> a set that has all the words from the top 10k words dataset (n)\n",
    "\n",
    "words_list\n",
    " --> used to iterate and have an orderly version of the words, this is used only for to make the features_vector  (1 x n)\n",
    "\n",
    "emails\n",
    "--> features (m x 1)\n",
    "\n",
    "lables \n",
    "--> 1 if spam, 0 if ham (m x 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rod',\n",
       " 'halloween',\n",
       " 'lawyer',\n",
       " 'maintained',\n",
       " 'kenya',\n",
       " 'header',\n",
       " 'detected',\n",
       " 'sv',\n",
       " 'toolkit',\n",
       " 'deadline',\n",
       " 'andale',\n",
       " 'samples',\n",
       " 'whilst',\n",
       " 'prague',\n",
       " 'locate',\n",
       " 'ranging',\n",
       " 'pending',\n",
       " 'louise',\n",
       " 'albany',\n",
       " 'warriors',\n",
       " 'guarantees',\n",
       " 'surplus',\n",
       " 'midnight',\n",
       " 'nav',\n",
       " 'attachments',\n",
       " 'signals',\n",
       " 'slowly',\n",
       " 'occurrence',\n",
       " 'hear',\n",
       " 'growth',\n",
       " 'oman',\n",
       " 'dx',\n",
       " 'levels',\n",
       " 'violin',\n",
       " 'fires',\n",
       " 'genres',\n",
       " 'circus',\n",
       " 'dui',\n",
       " 'agents',\n",
       " 'all',\n",
       " 'drivers',\n",
       " 'annual',\n",
       " 'bug',\n",
       " 'lucia',\n",
       " 'brad',\n",
       " 'differently',\n",
       " 'term',\n",
       " 'aberdeen',\n",
       " 'configurations',\n",
       " 'escort',\n",
       " 'simple',\n",
       " 'integration',\n",
       " 'victor',\n",
       " 'viking',\n",
       " 'worlds',\n",
       " 'light',\n",
       " 'many',\n",
       " 'smithsonian',\n",
       " 'urge',\n",
       " 'kill',\n",
       " 'separately',\n",
       " 'curious',\n",
       " 'exams',\n",
       " 'keywords',\n",
       " 'fa',\n",
       " 'hide',\n",
       " 'personally',\n",
       " 'aaron',\n",
       " 'isbn',\n",
       " 'buses',\n",
       " 'prev',\n",
       " 'ends',\n",
       " 'polyphonic',\n",
       " 'generating',\n",
       " 'examines',\n",
       " 'advice',\n",
       " 'launched',\n",
       " 'creations',\n",
       " 'affair',\n",
       " 'seminar',\n",
       " 'designer',\n",
       " 'relates',\n",
       " 'knowing',\n",
       " 'mason',\n",
       " 'posing',\n",
       " 'bloggers',\n",
       " 'parcel',\n",
       " 'false',\n",
       " 'arrangements',\n",
       " 'supreme',\n",
       " 'patches',\n",
       " 'consumer',\n",
       " 'blessed',\n",
       " 'tears',\n",
       " 'ministries',\n",
       " 'bulgaria',\n",
       " 'lexus',\n",
       " 'stretch',\n",
       " 'poster',\n",
       " 'factory',\n",
       " 'chronic',\n",
       " 'moms',\n",
       " 'taylor',\n",
       " 'biological',\n",
       " 'scuba',\n",
       " 'ohio',\n",
       " 'wires',\n",
       " 'states',\n",
       " 'radar',\n",
       " 'tea',\n",
       " 'pipeline',\n",
       " 'socket',\n",
       " 'indians',\n",
       " 'tommy',\n",
       " 'tsunami',\n",
       " 'rent',\n",
       " 'hints',\n",
       " 'topic',\n",
       " 'established',\n",
       " 'disputes',\n",
       " 'ate',\n",
       " 'press',\n",
       " 'phd',\n",
       " 'schedules',\n",
       " 'wales',\n",
       " 'dramatic',\n",
       " 'counting',\n",
       " 'skating',\n",
       " 'possess',\n",
       " 'fonts',\n",
       " 'enclosed',\n",
       " 'skirts',\n",
       " 'deal',\n",
       " 'notices',\n",
       " 'ranking',\n",
       " 'desktop',\n",
       " 'east',\n",
       " 'accreditation',\n",
       " 'collaborative',\n",
       " 'hunt',\n",
       " 'such',\n",
       " 'jonathan',\n",
       " 'loan',\n",
       " 'happened',\n",
       " 'during',\n",
       " 'surprised',\n",
       " 'skilled',\n",
       " 'ftp',\n",
       " 'iceland',\n",
       " 'authorities',\n",
       " 'actor',\n",
       " 'situated',\n",
       " 'worldcat',\n",
       " 'danny',\n",
       " 'rrp',\n",
       " 'willing',\n",
       " 'escorts',\n",
       " 'ratios',\n",
       " 'liabilities',\n",
       " 'ralph',\n",
       " 'ocean',\n",
       " 'pt',\n",
       " 'xl',\n",
       " 'humidity',\n",
       " 'appropriations',\n",
       " 'photography',\n",
       " 'wishes',\n",
       " 'place',\n",
       " 'matt',\n",
       " 'stem',\n",
       " 'color',\n",
       " 'filing',\n",
       " 'bugs',\n",
       " 'moderator',\n",
       " 'editorials',\n",
       " 'jesus',\n",
       " 'eden',\n",
       " 'temporal',\n",
       " 'mozambique',\n",
       " 'assessment',\n",
       " 'sponsorship',\n",
       " 'shark',\n",
       " 'excerpt',\n",
       " 'publicity',\n",
       " 'soundtrack',\n",
       " 'alot',\n",
       " 'peninsula',\n",
       " 'hometown',\n",
       " 'believes',\n",
       " 'cover',\n",
       " 'elect',\n",
       " 'star',\n",
       " 'punk',\n",
       " 'poet',\n",
       " 'tales',\n",
       " 'iso',\n",
       " 'pamela',\n",
       " 'compared',\n",
       " 'employees',\n",
       " 'vitamins',\n",
       " 'hawk',\n",
       " 'april',\n",
       " 'entry',\n",
       " 'cameron',\n",
       " 'parking',\n",
       " 'ministry',\n",
       " 'chaos',\n",
       " 'potatoes',\n",
       " 'lawyers',\n",
       " 'processors',\n",
       " 'awarded',\n",
       " 'mtv',\n",
       " 'boats',\n",
       " 'manuals',\n",
       " 'been',\n",
       " 'declared',\n",
       " 'announce',\n",
       " 'when',\n",
       " 'thats',\n",
       " 'voted',\n",
       " 'hypothetical',\n",
       " 'automation',\n",
       " 'receptor',\n",
       " 'earn',\n",
       " 'knives',\n",
       " 'mint',\n",
       " 'realm',\n",
       " 'vocational',\n",
       " 'rule',\n",
       " 'testimony',\n",
       " 'univ',\n",
       " 'cdt',\n",
       " 'buttons',\n",
       " 'spare',\n",
       " 'eh',\n",
       " 'neutral',\n",
       " 'heater',\n",
       " 'aggregate',\n",
       " 'exclude',\n",
       " 'macromedia',\n",
       " 'leo',\n",
       " 'limitation',\n",
       " 'excel',\n",
       " 'laid',\n",
       " 'badge',\n",
       " 'complexity',\n",
       " 'aye',\n",
       " 'trucks',\n",
       " 'dicke',\n",
       " 'paintings',\n",
       " 'consultants',\n",
       " 'wav',\n",
       " 'indonesia',\n",
       " 'tue',\n",
       " 'ton',\n",
       " 'latino',\n",
       " 'born',\n",
       " 'hardcore',\n",
       " 'assisted',\n",
       " 'controller',\n",
       " 'underlying',\n",
       " 'aerospace',\n",
       " 'te',\n",
       " 'job',\n",
       " 'junction',\n",
       " 'listed',\n",
       " 'plates',\n",
       " 'eminem',\n",
       " 'composition',\n",
       " 'hand',\n",
       " 'stud',\n",
       " 'improve',\n",
       " 'forward',\n",
       " 'configuration',\n",
       " 'arrived',\n",
       " 'communications',\n",
       " 'cakes',\n",
       " 'whats',\n",
       " 'legend',\n",
       " 'dispute',\n",
       " 'satin',\n",
       " 'shares',\n",
       " 'behalf',\n",
       " 'maryland',\n",
       " 'raise',\n",
       " 'tutorial',\n",
       " 'connecting',\n",
       " 'toshiba',\n",
       " 'bt',\n",
       " 'promise',\n",
       " 'shock',\n",
       " 'matthew',\n",
       " 'dozen',\n",
       " 'impose',\n",
       " 'managers',\n",
       " 'tissue',\n",
       " 'interaction',\n",
       " 'recommends',\n",
       " 'encouraged',\n",
       " 'piss',\n",
       " 'moreover',\n",
       " 'tabs',\n",
       " 'websites',\n",
       " 'abstract',\n",
       " 'accounts',\n",
       " 'kentucky',\n",
       " 'vancouver',\n",
       " 'cyprus',\n",
       " 'legitimate',\n",
       " 'cognitive',\n",
       " 'tickets',\n",
       " 'overseas',\n",
       " 'hu',\n",
       " 'amino',\n",
       " 'batteries',\n",
       " 'town',\n",
       " 'suffer',\n",
       " 'buzz',\n",
       " 'monica',\n",
       " 'needed',\n",
       " 'throw',\n",
       " 'lawrence',\n",
       " 'expert',\n",
       " 'deutsche',\n",
       " 'pairs',\n",
       " 'designs',\n",
       " 'formula',\n",
       " 'sends',\n",
       " 'nirvana',\n",
       " 'ugly',\n",
       " 'coverage',\n",
       " 'jd',\n",
       " 'discusses',\n",
       " 'photograph',\n",
       " 'ui',\n",
       " 'gras',\n",
       " 'founded',\n",
       " 'voyeur',\n",
       " 'pointed',\n",
       " 'passive',\n",
       " 'surveys',\n",
       " 'force',\n",
       " 'kijiji',\n",
       " 'attacks',\n",
       " 'courier',\n",
       " 'remembered',\n",
       " 'among',\n",
       " 'overview',\n",
       " 'seasons',\n",
       " 'amanda',\n",
       " 'breakfast',\n",
       " 'symphony',\n",
       " 'ka',\n",
       " 'immigration',\n",
       " 'training',\n",
       " 'territory',\n",
       " 'invalid',\n",
       " 'reflection',\n",
       " 'bonus',\n",
       " 'remarks',\n",
       " 'indian',\n",
       " 'groups',\n",
       " 'venue',\n",
       " 'decade',\n",
       " 'jamie',\n",
       " 'spirit',\n",
       " 'formats',\n",
       " 'christmas',\n",
       " 'oe',\n",
       " 'shot',\n",
       " 'areas',\n",
       " 'anonymous',\n",
       " 'greenhouse',\n",
       " 'improved',\n",
       " 'nursing',\n",
       " 'roland',\n",
       " 'input',\n",
       " 'hay',\n",
       " 'zimbabwe',\n",
       " 'paul',\n",
       " 'murphy',\n",
       " 'rider',\n",
       " 'hook',\n",
       " 'breaking',\n",
       " 'escape',\n",
       " 'sally',\n",
       " 'administration',\n",
       " 'labels',\n",
       " 'personals',\n",
       " 'mineral',\n",
       " 'notify',\n",
       " 'pros',\n",
       " 'drove',\n",
       " 'kw',\n",
       " 'mt',\n",
       " 'independence',\n",
       " 'discounted',\n",
       " 'precipitation',\n",
       " 'oil',\n",
       " 'fighter',\n",
       " 'shame',\n",
       " 'rankings',\n",
       " 'exceed',\n",
       " 'thru',\n",
       " 'modern',\n",
       " 'cvs',\n",
       " 'residential',\n",
       " 'normally',\n",
       " 'port',\n",
       " 'bee',\n",
       " 'degrees',\n",
       " 'bowling',\n",
       " 'category',\n",
       " 'dan',\n",
       " 'exceptions',\n",
       " 'cuba',\n",
       " 'cleaning',\n",
       " 'cookies',\n",
       " 'self',\n",
       " 'symbol',\n",
       " 'pages',\n",
       " 'arising',\n",
       " 'r',\n",
       " 'guests',\n",
       " 'spoken',\n",
       " 'stuffed',\n",
       " 'electronic',\n",
       " 'journalists',\n",
       " 'geographical',\n",
       " 'alexander',\n",
       " 'terry',\n",
       " 'arabia',\n",
       " 'armor',\n",
       " 'layout',\n",
       " 'basketball',\n",
       " 'intel',\n",
       " 'objective',\n",
       " 'lincoln',\n",
       " 'islam',\n",
       " 'radical',\n",
       " 'interpretation',\n",
       " 'exchange',\n",
       " 'idle',\n",
       " 'gcc',\n",
       " 'ty',\n",
       " 'sony',\n",
       " 'assessed',\n",
       " 'watch',\n",
       " 'numbers',\n",
       " 'counts',\n",
       " 'joyce',\n",
       " 'determining',\n",
       " 'little',\n",
       " 'weapons',\n",
       " 'silver',\n",
       " 'measurement',\n",
       " 'img',\n",
       " 'oriented',\n",
       " 'instances',\n",
       " 'governmental',\n",
       " 'des',\n",
       " 'painting',\n",
       " 'attitude',\n",
       " 'elderly',\n",
       " 'russian',\n",
       " 'remain',\n",
       " 'measurements',\n",
       " 'filename',\n",
       " 'donors',\n",
       " 'url',\n",
       " 'dis',\n",
       " 'ward',\n",
       " 'correct',\n",
       " 'lack',\n",
       " 'compact',\n",
       " 'eco',\n",
       " 'beastiality',\n",
       " 'installations',\n",
       " 'viewpicture',\n",
       " 'handmade',\n",
       " 'acne',\n",
       " 'ultimately',\n",
       " 'maine',\n",
       " 'ic',\n",
       " 'uploaded',\n",
       " 'faces',\n",
       " 'concord',\n",
       " 'printable',\n",
       " 'abstracts',\n",
       " 'initiative',\n",
       " 'hd',\n",
       " 'winners',\n",
       " 'mrna',\n",
       " 'rca',\n",
       " 'telephone',\n",
       " 'pointer',\n",
       " 'assumptions',\n",
       " 'spend',\n",
       " 'imagine',\n",
       " 'register',\n",
       " 'mud',\n",
       " 'pillow',\n",
       " 'happen',\n",
       " 'ins',\n",
       " 'newspapers',\n",
       " 'california',\n",
       " 'stone',\n",
       " 'watched',\n",
       " 'incentives',\n",
       " 'avon',\n",
       " 'lease',\n",
       " 'h',\n",
       " 'trek',\n",
       " 'seen',\n",
       " 'midwest',\n",
       " 'avatar',\n",
       " 'courage',\n",
       " 'contributions',\n",
       " 'repair',\n",
       " 'boot',\n",
       " 'fate',\n",
       " 'ent',\n",
       " 'proposals',\n",
       " 'lecture',\n",
       " 'followed',\n",
       " 'wy',\n",
       " 'delivery',\n",
       " 'peeing',\n",
       " 'promptly',\n",
       " 'completion',\n",
       " 'intensive',\n",
       " 'hero',\n",
       " 'belly',\n",
       " 'lion',\n",
       " 'trivia',\n",
       " 'adapted',\n",
       " 'developed',\n",
       " 'victorian',\n",
       " 'strategies',\n",
       " 'steel',\n",
       " 'mechanics',\n",
       " 'reception',\n",
       " 'bizrate',\n",
       " 'viruses',\n",
       " 'gas',\n",
       " 'st',\n",
       " 'fotos',\n",
       " 'finger',\n",
       " 'pp',\n",
       " 'page',\n",
       " 'm',\n",
       " 'distinction',\n",
       " 'david',\n",
       " 'judges',\n",
       " 'blake',\n",
       " 'surround',\n",
       " 'morrison',\n",
       " 'societies',\n",
       " 'invention',\n",
       " 'resistant',\n",
       " 'merger',\n",
       " 'desire',\n",
       " 'candles',\n",
       " 'discussed',\n",
       " 'hop',\n",
       " 'rock',\n",
       " 'fuck',\n",
       " 'hart',\n",
       " 'tradition',\n",
       " 'joseph',\n",
       " 'studio',\n",
       " 'organization',\n",
       " 'sacramento',\n",
       " 'campaigns',\n",
       " 'arrest',\n",
       " 'guru',\n",
       " 'calculators',\n",
       " 'florence',\n",
       " 'hi',\n",
       " 'tobacco',\n",
       " 'consider',\n",
       " 'gain',\n",
       " 'insert',\n",
       " 'mo',\n",
       " 'glass',\n",
       " 'providers',\n",
       " 'oxford',\n",
       " 'directly',\n",
       " 'looked',\n",
       " 'wire',\n",
       " 'ion',\n",
       " 'acute',\n",
       " 'bits',\n",
       " 'remix',\n",
       " 'enormous',\n",
       " 'ch',\n",
       " 'beginning',\n",
       " 'proud',\n",
       " 'scott',\n",
       " 'span',\n",
       " 'alias',\n",
       " 'meta',\n",
       " 'seat',\n",
       " 'allows',\n",
       " 'll',\n",
       " 'chester',\n",
       " 'metal',\n",
       " 'petite',\n",
       " 'computed',\n",
       " 'suite',\n",
       " 'changing',\n",
       " 'properties',\n",
       " 'nationally',\n",
       " 'nearest',\n",
       " 'disks',\n",
       " 'breach',\n",
       " 'partnerships',\n",
       " 'offshore',\n",
       " 'coins',\n",
       " 'personal',\n",
       " 'luxury',\n",
       " 'pharmacies',\n",
       " 'harvest',\n",
       " 'odds',\n",
       " 'village',\n",
       " 'particularly',\n",
       " 'lawsuit',\n",
       " 'paris',\n",
       " 'aqua',\n",
       " 'tit',\n",
       " 'climate',\n",
       " 'amplifier',\n",
       " 'paste',\n",
       " 'her',\n",
       " 'went',\n",
       " 'isaac',\n",
       " 'excited',\n",
       " 'arrow',\n",
       " 'elevation',\n",
       " 'figures',\n",
       " 'yrs',\n",
       " 'q',\n",
       " 'dover',\n",
       " 'clay',\n",
       " 'opportunities',\n",
       " 'criticism',\n",
       " 'benchmark',\n",
       " 'xnxx',\n",
       " 'yet',\n",
       " 'ripe',\n",
       " 'devon',\n",
       " 'immediate',\n",
       " 'keith',\n",
       " 'points',\n",
       " 'faq',\n",
       " 'mom',\n",
       " 'textiles',\n",
       " 'homework',\n",
       " 'medicine',\n",
       " 'include',\n",
       " 'release',\n",
       " 'helping',\n",
       " 'destiny',\n",
       " 'arthur',\n",
       " 'fg',\n",
       " 'nbc',\n",
       " 'inexpensive',\n",
       " 'const',\n",
       " 'transcripts',\n",
       " 'ebooks',\n",
       " 'cialis',\n",
       " 'jacob',\n",
       " 'over',\n",
       " 'warehouse',\n",
       " 'give',\n",
       " 'ps',\n",
       " 'trails',\n",
       " 'samba',\n",
       " 'penalties',\n",
       " 'political',\n",
       " 'selection',\n",
       " 'attention',\n",
       " 'severe',\n",
       " 'identifier',\n",
       " 'brazilian',\n",
       " 'ii',\n",
       " 'drink',\n",
       " 'illegal',\n",
       " 'settings',\n",
       " 'metropolitan',\n",
       " 'browse',\n",
       " 'bow',\n",
       " 'fraction',\n",
       " 'applications',\n",
       " 'refuse',\n",
       " 'dial',\n",
       " 'approximate',\n",
       " 'sao',\n",
       " 'displayed',\n",
       " 'integral',\n",
       " 'kenneth',\n",
       " 'protest',\n",
       " 'united',\n",
       " 'change',\n",
       " 'sampling',\n",
       " 'pregnant',\n",
       " 'la',\n",
       " 'dynamic',\n",
       " 'introducing',\n",
       " 'sacred',\n",
       " 'sauce',\n",
       " 'wang',\n",
       " 'hepatitis',\n",
       " 'evening',\n",
       " 'forever',\n",
       " 'erotica',\n",
       " 'zoom',\n",
       " 'decide',\n",
       " 'nov',\n",
       " 'releases',\n",
       " 'cambridge',\n",
       " 'tourism',\n",
       " 'respect',\n",
       " 'ccd',\n",
       " 'involves',\n",
       " 'coordinate',\n",
       " 'bumper',\n",
       " 'ford',\n",
       " 'instrumentation',\n",
       " 'balloon',\n",
       " 'fin',\n",
       " 'fun',\n",
       " 'outlets',\n",
       " 'curriculum',\n",
       " 'miscellaneous',\n",
       " 'mary',\n",
       " 'primary',\n",
       " 'identify',\n",
       " 'taught',\n",
       " 'intersection',\n",
       " 'double',\n",
       " 'steven',\n",
       " 'broader',\n",
       " 'tragedy',\n",
       " 'drive',\n",
       " 'banana',\n",
       " 'yeast',\n",
       " 'route',\n",
       " 'usual',\n",
       " 'country',\n",
       " 'ignored',\n",
       " 'hip',\n",
       " 'airplane',\n",
       " 'delay',\n",
       " 'grants',\n",
       " 'nominations',\n",
       " 'difference',\n",
       " 'fc',\n",
       " 'supplements',\n",
       " 'fact',\n",
       " 'law',\n",
       " 'races',\n",
       " 'ft',\n",
       " 'thesaurus',\n",
       " 'accomplished',\n",
       " 'diseases',\n",
       " 'tx',\n",
       " 'myrtle',\n",
       " 'between',\n",
       " 'vermont',\n",
       " 'environment',\n",
       " 'bought',\n",
       " 'times',\n",
       " 'vcr',\n",
       " 'implement',\n",
       " 'envelope',\n",
       " 'elimination',\n",
       " 'sites',\n",
       " 'athletic',\n",
       " 'advisor',\n",
       " 'celebs',\n",
       " 'jet',\n",
       " 'letter',\n",
       " 'fed',\n",
       " 'poly',\n",
       " 'oo',\n",
       " 'benjamin',\n",
       " 'dec',\n",
       " 'auckland',\n",
       " 'derived',\n",
       " 'park',\n",
       " 'exciting',\n",
       " 'ethical',\n",
       " 'joins',\n",
       " 'archive',\n",
       " 'odd',\n",
       " 'carriers',\n",
       " 'language',\n",
       " 'repeated',\n",
       " 'holdings',\n",
       " 'wake',\n",
       " 'lamp',\n",
       " 'retro',\n",
       " 'discount',\n",
       " 'venture',\n",
       " 'teen',\n",
       " 'stupid',\n",
       " 'handles',\n",
       " 'cancer',\n",
       " 'piece',\n",
       " 'reduction',\n",
       " 'nice',\n",
       " 'actors',\n",
       " 'demonstration',\n",
       " 'dl',\n",
       " 'quizzes',\n",
       " 'prove',\n",
       " 'breeding',\n",
       " 'yukon',\n",
       " 'reliable',\n",
       " 'amsterdam',\n",
       " 'tap',\n",
       " 'sox',\n",
       " 'situations',\n",
       " 'jose',\n",
       " 'chancellor',\n",
       " 'powered',\n",
       " 'kills',\n",
       " 'spatial',\n",
       " 'babies',\n",
       " 'ae',\n",
       " 'multi',\n",
       " 'integrity',\n",
       " 'manufacturer',\n",
       " 'nd',\n",
       " 'zus',\n",
       " 'sailing',\n",
       " 'assuming',\n",
       " 'munich',\n",
       " 'dock',\n",
       " 'accredited',\n",
       " 'fundamentals',\n",
       " 'partners',\n",
       " 'kong',\n",
       " 'concentration',\n",
       " 'noon',\n",
       " 'pendant',\n",
       " 'settle',\n",
       " 'revised',\n",
       " 'birmingham',\n",
       " 'exec',\n",
       " 'manufacturing',\n",
       " 'attended',\n",
       " 'infection',\n",
       " 'cluster',\n",
       " 'dick',\n",
       " 'boss',\n",
       " 'forums',\n",
       " 'identical',\n",
       " 'reply',\n",
       " 'freedom',\n",
       " 'milan',\n",
       " 'ozone',\n",
       " 'garcia',\n",
       " 'cj',\n",
       " 'reprints',\n",
       " 'tom',\n",
       " 'roll',\n",
       " 'madison',\n",
       " 'french',\n",
       " 'extreme',\n",
       " 'served',\n",
       " 'tenant',\n",
       " 'irish',\n",
       " 'howto',\n",
       " 'addition',\n",
       " 'avg',\n",
       " 'conspiracy',\n",
       " 'removable',\n",
       " 'require',\n",
       " 'portuguese',\n",
       " 'bureau',\n",
       " 'showed',\n",
       " 'gore',\n",
       " 'underground',\n",
       " 'bitch',\n",
       " 'hits',\n",
       " 'oldest',\n",
       " 'nextel',\n",
       " 'fruit',\n",
       " 'saturday',\n",
       " 'stops',\n",
       " 'presently',\n",
       " 'cork',\n",
       " 'superintendent',\n",
       " 'pharmaceutical',\n",
       " 'cod',\n",
       " 'diabetes',\n",
       " 'strategy',\n",
       " 'copyrights',\n",
       " 'margin',\n",
       " 'transfer',\n",
       " 'dimensional',\n",
       " 'agency',\n",
       " 'nursery',\n",
       " 'fioricet',\n",
       " 'clinics',\n",
       " 'herb',\n",
       " 'section',\n",
       " 'played',\n",
       " 'ca',\n",
       " 'flashers',\n",
       " 'cited',\n",
       " 'pads',\n",
       " 'northwest',\n",
       " 'gender',\n",
       " 'put',\n",
       " 'compatible',\n",
       " 'ink',\n",
       " 'grow',\n",
       " 'pressed',\n",
       " 'computational',\n",
       " 'collar',\n",
       " 'current',\n",
       " 'love',\n",
       " 'july',\n",
       " 'louisiana',\n",
       " 'cancellation',\n",
       " 'cho',\n",
       " 'recruitment',\n",
       " 'attacked',\n",
       " 'dirt',\n",
       " 'diverse',\n",
       " 'butterfly',\n",
       " 'unexpected',\n",
       " 'ordinance',\n",
       " 'ext',\n",
       " 'robin',\n",
       " 'dam',\n",
       " 'treaty',\n",
       " 'planet',\n",
       " 'leaders',\n",
       " 'implied',\n",
       " 'wanting',\n",
       " 'club',\n",
       " 'kitty',\n",
       " 'sociology',\n",
       " 'bone',\n",
       " 'assumes',\n",
       " 'partial',\n",
       " 'single',\n",
       " 'ng',\n",
       " 'how',\n",
       " 'madagascar',\n",
       " 'popular',\n",
       " 'pirates',\n",
       " 'helicopter',\n",
       " 'watts',\n",
       " 'approval',\n",
       " 'new',\n",
       " 'checks',\n",
       " 'tables',\n",
       " 'assure',\n",
       " 'ag',\n",
       " 'competitive',\n",
       " 'effectiveness',\n",
       " 'tested',\n",
       " 'mirror',\n",
       " 'nearby',\n",
       " 'practitioner',\n",
       " 'climbing',\n",
       " 'forces',\n",
       " 'thumbzilla',\n",
       " 'gps',\n",
       " 'headset',\n",
       " 'f',\n",
       " 'bg',\n",
       " 'suggests',\n",
       " 'llc',\n",
       " 'continues',\n",
       " 'directory',\n",
       " 'heel',\n",
       " 'render',\n",
       " 'potter',\n",
       " 'streets',\n",
       " 'tan',\n",
       " 'sentences',\n",
       " 'at',\n",
       " 'consisting',\n",
       " 'richard',\n",
       " 'valuation',\n",
       " 'evil',\n",
       " 'packet',\n",
       " 'weapon',\n",
       " 'affected',\n",
       " 'trinidad',\n",
       " 'servers',\n",
       " 'approaches',\n",
       " 'interactive',\n",
       " 'cafe',\n",
       " ...}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = set(np.genfromtxt(\"google-10000-english-usa.txt\", dtype=str, delimiter=\"\\n\"))\n",
    "\n",
    "words_list = sorted(words)\n",
    "\n",
    "emails = pd.read_csv('emails.csv')\n",
    "\n",
    "lables = emails['spam']\n",
    "\n",
    "emails = emails.drop('spam', axis=1)\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "email_words \n",
    "--> a numpy array of python sets. each cell of the numpy array corrosponds to a different training example. each set in that cell contains all the unique words used in each email corrosponding to the cell. (contains m sets in m cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_words = np.array([{'a'}], dtype=object)\n",
    "\n",
    "for email in emails['text']:\n",
    "        email_words = np.append(email_words, np.array([set(email.lower().split())], dtype=object), axis=0)\n",
    "\n",
    "\n",
    "email_words = np.delete(email_words, 0, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_vector is an (m x n) sparce matrix. each row corresponds to a different training example (different email). each column corrosponds to a different word in the word_list defined before. if the training example contains the word in its set, we mark the corrosponding column of the email's row as 1, if it does not contain the word in its set, we mark it as 0. \n",
    "\n",
    "we use a sparce matrix beacause majority of the values in the feature_vector is going to be 0s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "email_words = set([])\n",
    "intersection_set = set([])\n",
    "\n",
    "feature_vector = csr_matrix((0, len(words_list)))\n",
    "\n",
    "for email in emails[\"text\"]:\n",
    "    email_words = set(email.lower().split())\n",
    "\n",
    "    intersection_set = email_words & words\n",
    "    \n",
    "    row = []\n",
    "\n",
    "    for word in words_list:\n",
    "        if word in intersection_set:\n",
    "            row.append(True) # appends 1\n",
    "        else:\n",
    "            row.append(False) # appends 0\n",
    "\n",
    "    \n",
    "    row = csr_matrix([row])\n",
    "    feature_vector = vstack([feature_vector, row])\n",
    "\n",
    "feature_vector = csr_matrix(feature_vector)\n",
    "\n",
    "feature_vector.toarray()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to seperate the ham and spam emails from the features (emails) datset. we do this beacuse naive bayes is a generative learning algorithm and not a discriminative learning algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_mail = feature_vector[lables.to_numpy() == 0]\n",
    "spam_mail = feature_vector[lables.to_numpy() == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_mail.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_mail.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the paramteres next: \n",
    "\n",
    "phi_y1 -->  P(Y = 1) probability of the email being a spam mail when selected at random from the dataset\n",
    "phi_y0 -->  P(Y = 0) probability of the email being a ham mail when selected at random from the dataset\n",
    "\n",
    "parameters_11 --> P(X~j~ = 1 | Y = 1) (1 x n)\n",
    "parameters_01 --> P(X~j~ = 0 | Y = 1) (1 x n)\n",
    "parameters_10 --> P(X~j~ = 1 | Y = 0) (1 x n)\n",
    "parameters_00 --> P(X~j~ = 0 | Y = 0) (1 x n)\n",
    "\n",
    "basically what the above 4 parameteres calculate is: \n",
    "what fraction of spam/ham emails had the word X~j~ in it? \n",
    "\n",
    "\n",
    "We will be using log probabilities instead of normal probabilities as it prevents underflow and also beacuse multiplication takes a lot of comutational power as compared to addition. \n",
    "\n",
    "since we will be using the log values of the parameters and the priors, we cannot let the values be 0, this would throw an error, we assign the 0 values to a very very tiny value (1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_y1 = spam_mail.shape[0] / (spam_mail.shape[0] + ham_mail.shape[0])\n",
    "\n",
    "parameters_11 = spam_mail.sum(axis=0) / spam_mail.shape[0]\n",
    "\n",
    "phi_y0 = 1 - phi_y1\n",
    "\n",
    "parameters_01 = 1 - parameters_11\n",
    "\n",
    "parameters_00 = ham_mail.sum(axis=0) / ham_mail.shape[0]\n",
    "\n",
    "parameters_10 = 1 - parameters_00\n",
    "\n",
    "# we cannot have 0 as a value in paramters, so i will make it a very tiny value isntead. \n",
    "\n",
    "parameters_11[parameters_11 == 0] = 1e-10\n",
    "parameters_00[parameters_00 == 0] = 1e-10\n",
    "parameters_01[parameters_01 == 0] = 1e-10\n",
    "parameters_10[parameters_10 == 0] = 1e-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[7.82798165e-01, 6.88073394e-03, 1.37614679e-03, ...,\n",
       "          6.88073394e-04, 6.88073394e-04, 1.00000000e-10]]),\n",
       " matrix([[0.27046784, 0.99853801, 1.        , ..., 1.        , 0.99926901,\n",
       "          1.        ]]),\n",
       " matrix([[0.21720183, 0.99311927, 0.99862385, ..., 0.99931193, 0.99931193,\n",
       "          1.        ]]),\n",
       " matrix([[7.29532164e-01, 1.46198830e-03, 1.00000000e-10, ...,\n",
       "          1.00000000e-10, 7.30994152e-04, 1.00000000e-10]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_00, parameters_01, parameters_10, parameters_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_11 = np.log(parameters_11)\n",
    "\n",
    "parameters_01 = np.log(parameters_01)   \n",
    "\n",
    "parameters_00 = np.log(parameters_00)   \n",
    "\n",
    "parameters_10 = np.log(parameters_10)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[ -0.24488039,  -4.97902995,  -6.58846787, ...,  -7.28161505,\n",
       "           -7.28161505, -23.02585093]]),\n",
       " matrix([[-1.30760209e+00, -1.46305805e-03,  0.00000000e+00, ...,\n",
       "           0.00000000e+00, -7.31261459e-04,  0.00000000e+00]]),\n",
       " matrix([[-1.52692824e+00, -6.90451535e-03, -1.37709455e-03, ...,\n",
       "          -6.88310226e-04, -6.88310226e-04,  0.00000000e+00]]),\n",
       " matrix([[ -0.31535182,  -6.52795792, -23.02585093, ..., -23.02585093,\n",
       "           -7.2211051 , -23.02585093]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_00, parameters_01, parameters_10, parameters_11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that paramters are calculated, we can take in our input: query_email. \n",
    "\n",
    "we again make a set of the unique words in the query_email and store it in query_email_words\n",
    "\n",
    "and we make a query_vector (1 x n) the same way we made the rows of the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_email = \"\"\" \n",
    "Subject: making room for \" summer interns \"  hello all :  i have some \" good \" news and i have some \" bad \" news . the \" good \" news  is that we are growing ! the \" bad \" news is that we are running out of space .  therefore , just for the summer we need to use jose ' s and joe ' s offices  on the 19 th floor for \" summer \" interns .  i will have to remove the extensions from those offices so they won ' t be  ringing all the time . however , we can put them back after the summer is  over .  i hope this is not too inconvenient for you , but with roman and jason needing  spaces on the 19 th floor , and another new hire coming on board , we are left  without any extra spaces .  thanks !  shirley\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query_email_words = set(query_email.lower().split())\n",
    "\n",
    "query_vector = []\n",
    "for word in words_list:\n",
    "    if word in query_email_words:\n",
    "        query_vector.append(1)\n",
    "    else:\n",
    "        query_vector.append(0)\n",
    "\n",
    "query_vector = csr_matrix([query_vector])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculating to give predictions:\n",
    "\n",
    "log_ham_prob --> log probability of the mail being ham\n",
    "log_spam_prob --> log probability of the mail being spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ham_prob = np.dot(query_vector.toarray(), parameters_10.T) + np.dot((1 - query_vector.toarray()), parameters_00.T) + np.log(phi_y0)\n",
    "\n",
    "\n",
    "log_spam_prob = np.dot(query_vector.toarray(), parameters_11.T) + np.dot((1 - query_vector.toarray()), parameters_01.T) + np.log(phi_y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalizing the log_ham_prob beacuse the number of spammy words is significantly lesser than the ham words. hence we tend to always have high log_ham_prob\n",
    "\n",
    "i came up with the number 500 after observing a few examples, there may be a better number for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-199.84669622]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_ham_prob /= 500\n",
    "\n",
    "log_ham_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-291.36752487]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_spam_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n"
     ]
    }
   ],
   "source": [
    "if log_spam_prob > log_ham_prob:\n",
    "    print(\"spam\")\n",
    "else:\n",
    "    print(\"ham\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
